{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c65cec58",
   "metadata": {},
   "source": [
    "# This notebook was used to create the TURL input representation tables and all necessary files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c998e455",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4b446158",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_path = '../../../../src/data/product'\n",
    "train_test_all_filtered_path = os.path.join(product_path, 'train_test_split/output_unfiltered_tables/large/after_manual_checking')\n",
    "files_representation_train = [file for file in os.listdir(os.path.join(train_test_all_filtered_path,'val_cleaned')) if file.endswith('.json.gz')]\n",
    "turl_input_path = os.path.join(product_path, 'TURL/input')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992c0e73",
   "metadata": {},
   "source": [
    "# Generate representations for rewritten and transposed versions. Attention: the generation has to be done seperatly by changing paths with test, val and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5488ef77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # use clean tables to get table representation for TURL rewritten\n",
    "train_representation=[]\n",
    "for zip_file in files_representation_train:\n",
    "    print('/{}'.format(zip_file))\n",
    "    df = pd.read_json(os.path.join(train_test_all_filtered_path,'test_cleaned') + '/{}'.format(zip_file), compression='gzip', lines=True)\n",
    "    if ('description' in df.columns)== False: # check if description column given\n",
    "        df['description'] = ''\n",
    "    if ('name' in df.columns)== False: # check if description column given\n",
    "        df['name'] = df.tokens\n",
    "    df_cleaned=df[df['cluster_id']!=-100].reset_index().drop(columns=['index'])\n",
    "    table_representation = [] #empty list for table\n",
    "    table_representation.append(zip_file)#append table id\n",
    "    table_representation.append('')#append page title -> not relevant\n",
    "    table_representation.append('')#append wikipedia page id -> not given\n",
    "    table_representation.append('')#append information about entity -> product, not relevant since same for all tables\n",
    "    table_representation.append('')#append table caption -> not given\n",
    "    table_representation.append(['name','description']) #append headers -> not sure if we should do that\n",
    "    all_rows_representation = []#representation of all rows\n",
    "    column_1_representation=[] # cell representation of column 1    \n",
    "    column_2_representation=[] # cell representation of column 1\n",
    "    for i in range(len(df_cleaned)):\n",
    "        column_1_representation.append([[i,0],[df_cleaned['row_id'][i],str(df_cleaned['name'][i])]])\n",
    "        column_2_representation.append([[i,1],[df_cleaned['row_id'][i],str(df_cleaned['description'][i])]])\n",
    "    all_rows_representation.append(column_1_representation)\n",
    "    all_rows_representation.append(column_2_representation) #append single column representation to representation of all rows\n",
    "    table_representation.append(all_rows_representation)#append it to representation of whole table\n",
    "    table_representation.append(df_cleaned['cluster_id'].apply(lambda x: [str(x)]).to_list())\n",
    "    train_representation.append(table_representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c43dac89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use clean tables to get table representation for TURL with transposed matrix\n",
    "train_representation=[] #representation of whole training set\n",
    "for zip_file in files_representation_train:\n",
    "    print('/{}'.format(zip_file))\n",
    "    df = pd.read_json(os.path.join(train_test_all_filtered_path,'val_cleaned') + '/{}'.format(zip_file), compression='gzip', lines=True)\n",
    "    if ('description' in df.columns)== False: # check if description column given\n",
    "        df['description'] = ''\n",
    "    if ('name' in df.columns)== False: # check if description column given\n",
    "        df['name'] = df.tokens\n",
    "    df['header']='Product'#empty string for headers\n",
    "    df_cleaned=df[df['cluster_id']!=-100].reset_index().drop(columns=['index'])#get rid of clusters with -100 as data is too much\n",
    "    table_representation = [] #empty list for table\n",
    "    table_representation.append(zip_file)#append table id\n",
    "    table_representation.append('')#append page title -> not relevant\n",
    "    table_representation.append('')#append wikipedia page id -> not given\n",
    "    table_representation.append('')#append information about entity -> product, not relevant since same for all tables\n",
    "    table_representation.append('')#append table caption -> not given\n",
    "    table_representation.append(df_cleaned['header'].to_list()) #append headers -> not sure if we should do that\n",
    "    all_rows_representation = []#representation of all rows\n",
    "    for i in range(len(df_cleaned)):\n",
    "        row_representation=[] # cell representation of single row\n",
    "        row_representation.append([[0,i],[df_cleaned['row_id'][i],str(df_cleaned['name'][i])]])\n",
    "        row_representation.append([[1,i],[df_cleaned['row_id'][i],str(df_cleaned['description'][i])]])\n",
    "        all_rows_representation.append(row_representation) #append single cell representation to representation of all rows\n",
    "    table_representation.append(all_rows_representation)#append it to representation of whole table\n",
    "    table_representation.append(df_cleaned['cluster_id'].apply(lambda x: [str(x)]).to_list())\n",
    "    train_representation.append(table_representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb29231b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #cannot read numpy integers in json -> encoder for saving\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "class NpEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        if isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return super(NpEncoder, self).default(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eed7b07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save data as json\n",
    "with open(os.path.join(turl_input_path, 'val_cleaned_representation_header_product_cleaned.json'), 'w') as f:\n",
    "    json.dump(json.dumps(train_representation, cls=NpEncoder), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87b88cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # generate type_vocab.txt\n",
    "df_train= pd.read_json(os.path.join(train_test_all_filtered_path,'train/concatenated_data/train_all_filtered_tables.json.gz'), compression='gzip', lines=True)\n",
    "df_test= pd.read_json(os.path.join(train_test_all_filtered_path,'test/concatenated_data/test_all_filtered_tables.json.gz'), compression='gzip', lines=True)\n",
    "df_val= pd.read_json(os.path.join(train_test_all_filtered_path,'val/concatenated_data/val_all_filtered_tables.json.gz'), compression='gzip', lines=True)\n",
    "ids = df_train.cluster_id.astype(str).unique()\n",
    "ids = np.append(ids,'-100')\n",
    "len(ids)\n",
    "#save list of cluster ids to txt file\n",
    "pd.DataFrame(ids).to_csv(path_or_buf=os.path.join(turl_input_path, 'type_vocab_clusters.txt'),sep='\\t', index=True, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f003a2f",
   "metadata": {},
   "source": [
    "# Generate representation for LocalBusiness data with the same setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a64cf49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_path = '../../../../src/data/LocalBusiness'\n",
    "train_test_all_filtered_path = os.path.join(product_path, 'Splitting_ManualCheck/Train_Validation_Test')\n",
    "files_representation_train = [file for file in os.listdir(os.path.join(train_test_all_filtered_path,'train_tables_cleaned')) if file.endswith('.csv')]\n",
    "files_representation_train = [file for file in os.listdir(os.path.join(train_test_all_filtered_path,'validation_tables_cleaned')) if file.endswith('.csv')]\n",
    "turl_input_path = os.path.join(product_path, 'TURL/input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8afa3404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # use clean tables to get table representation for TURL rewritten without progressbar\n",
    "train_representation=[]\n",
    "for zip_file in files_representation_train:\n",
    "    print('/{}'.format(zip_file))\n",
    "    df = pd.read_csv(os.path.join(train_test_all_filtered_path,'validation_tables_cleaned') + '/{}'.format(zip_file))\n",
    "    if ('name' in df.columns)== False: # check if description column given\n",
    "        df['name'] = df.page_url\n",
    "    if df[df['cluster_id']!=-100].empty == False: #check if table is empty\n",
    "        df_cleaned=df[df['cluster_id']!=-100].reset_index().drop(columns=['index'])\n",
    "        table_representation = [] #empty list for table\n",
    "        table_representation.append(zip_file)#append table id\n",
    "        table_representation.append('')#append page title -> not relevant\n",
    "        table_representation.append('')#append wikipedia page id -> not given\n",
    "        table_representation.append('')#append information about entity -> product, not relevant since same for all tables\n",
    "        table_representation.append('')#append table caption -> not given\n",
    "        table_representation.append(['name']) #append headers -> not sure if we should do that\n",
    "        all_rows_representation = []#representation of all rows\n",
    "        column_1_representation=[] # cell representation of column 1    \n",
    "        for i in range(len(df_cleaned)):\n",
    "            column_1_representation.append([[i,0],[df_cleaned.index[i],str(df_cleaned['name'][i])]])\n",
    "        all_rows_representation.append(column_1_representation)#append single column representation to representation of all rows\n",
    "        table_representation.append(all_rows_representation)#append it to representation of whole table\n",
    "        table_representation.append(df_cleaned['cluster_id'].apply(lambda x: [str(x)]).to_list())\n",
    "        train_representation.append(table_representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d166024",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # use clean tables to get table representation for TURL with transposed matrix\n",
    "train_representation=[] #representation of whole training set\n",
    "for zip_file in files_representation_train:\n",
    "    print('/{}'.format(zip_file))\n",
    "    df = pd.read_csv(os.path.join(train_test_all_filtered_path,'train_tables_cleaned') + '/{}'.format(zip_file))\n",
    "    if ('name' in df.columns)== False: # check if description column given\n",
    "        df['name'] = df.page_url\n",
    "    df['header']='LocalBusiness'#empty string for headers\n",
    "    if df[df['cluster_id']!=-100].empty == False: #check if table is empty\n",
    "        df_cleaned=df[df['cluster_id']!=-100].reset_index().drop(columns=['index'])#get rid of clusters with -100 as data is too much\n",
    "        table_representation = [] #empty list for table\n",
    "        table_representation.append(zip_file)#append table id\n",
    "        table_representation.append('')#append page title -> not relevant\n",
    "        table_representation.append('')#append wikipedia page id -> not given\n",
    "        table_representation.append('')#append information about entity -> product, not relevant since same for all tables\n",
    "        table_representation.append('')#append table caption -> not given\n",
    "        table_representation.append(df_cleaned['header'].to_list()) #append headers -> not sure if we should do that\n",
    "        all_rows_representation = []#representation of all rows\n",
    "        for i in range(len(df_cleaned)):\n",
    "            row_representation=[] # cell representation of single row\n",
    "            row_representation.append([[0,i],[df_cleaned.index[i],str(df_cleaned['name'][i])]])\n",
    "            all_rows_representation.append(row_representation) #append single cell representation to representation of all rows\n",
    "        table_representation.append(all_rows_representation)#append it to representation of whole table\n",
    "        table_representation.append(df_cleaned['cluster_id'].apply(lambda x: [str(x)]).to_list())\n",
    "        train_representation.append(table_representation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b850c8",
   "metadata": {},
   "source": [
    "# Save as json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b8824b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cannot read numpy integers in json -> encoder for saving\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "class NpEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        if isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return super(NpEncoder, self).default(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30d4a0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(turl_input_path, 'test_representation_rewritten_cleaned_lb.json'), 'w') as f:\n",
    "    json.dump(json.dumps(train_representation, cls=NpEncoder), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "30560e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(turl_input_path, 'val_cleaned_representation_rewritten_cleaned_lb.json'), 'w') as f:\n",
    "    json.dump(json.dumps(train_representation, cls=NpEncoder), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37d814c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate type vocab as list of clusters for input in TURL\n",
    "cluster_list=[]\n",
    "files_representation_train = [file for file in os.listdir(os.path.join(train_test_all_filtered_path,'test tables')) if file.endswith('.csv')]\n",
    "for zip_file in files_representation_train:\n",
    "    print('/{}'.format(zip_file))\n",
    "    df = pd.read_csv(os.path.join(train_test_all_filtered_path,'test tables') + '/{}'.format(zip_file))\n",
    "    cluster_list.extend(df['cluster_id'].tolist())\n",
    "    # get only clusters that are unique\n",
    "unique_clusters = np.unique(cluster_list)\n",
    "unique_clusters = np.delete(unique_clusters, 0)\n",
    "#save list of cluster ids to txt file\n",
    "pd.DataFrame(unique_clusters.astype(str)).to_csv(path_or_buf=os.path.join(turl_input_path, 'type_vocab_clusters_lb.txt'),sep='\\t', index=True, header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}