{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0cbeb3b",
   "metadata": {},
   "source": [
    "# This notebook evaluates the predictions of the TinyBert Baseline output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc2b3b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "676dcbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get path information\n",
    "product_path = '../../../../src/data/product'\n",
    "train_test_all_filtered_path_2 = os.path.join(product_path, 'train_test_split/output_unfiltered_tables/large/after_manual_checking/baselines')\n",
    "data_path = '../../../../src/data'\n",
    "mapping_corpus_path_2 = data_path + r'/product/lspc2020_to_tablecorpus/Cleaned'\n",
    "train_test_all_filtered_path = os.path.join(product_path, 'train_test_split/output_unfiltered_tables/large/after_manual_checking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1873fb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the TinyBert results\n",
    "res = pd.read_csv('../Baseline/TinyBert_Results/predict_results_None.txt', sep='\\t')\n",
    "res.drop('index', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5229136e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the test set\n",
    "real = pd.read_csv(os.path.join(train_test_all_filtered_path_2,'df_test.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "683875a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = pd.concat([real,res], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09498df1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8329422806194275"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(real.label, res.prediction, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3117677",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_scores = f1_score(real.label, res.prediction, average=None, labels=real.label)\n",
    "f1_scores_with_labels = {label:score for label,score in zip(real.label, f1_scores)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90cb2007",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_f1 = pd.DataFrame.from_dict(f1_scores_with_labels, orient=\"index\").reset_index().rename(columns={'index':'label',0:'f1'}).sort_values(by=['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e554094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get information on label, prediction, and cluster itself in one table\n",
    "#df_join_sentence = pd.merge(df_f1, real.drop_duplicates(subset=['label']), how=\"left\", on=[\"label\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ff3eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get information on label, prediction, and cluster itself in one table\n",
    "#pd.merge(final, df_f1, how=\"left\", on=[\"label\", \"label\"]).sort_values(by=['label']).to_excel('f1_per_cluster_baseline.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b359a94a",
   "metadata": {},
   "source": [
    "# F1 Scores for different domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b926b74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "domains=['Bikes','Cars','Clothes','Drugstore','Electronics','Random','Technology','Tools']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff6f4b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all clusters with information\n",
    "combined_csv_data = pd.concat([pd.read_csv(os.path.join(mapping_corpus_path_2, f\"{file}_cluster_8_tables.csv\")) for file in domains]).drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "425f6686",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get cluster_ids\n",
    "cluster_list=[]\n",
    "files_representation_train = [file for file in os.listdir(os.path.join(train_test_all_filtered_path,'train_cleaned')) if file.endswith('.json.gz')]\n",
    "for zip_file in files_representation_train:\n",
    "    df = pd.read_json(os.path.join(train_test_all_filtered_path,'train_cleaned') + '/{}'.format(zip_file), compression='gzip', lines=True)\n",
    "    cluster_list.extend(df['cluster_id'].tolist())\n",
    "    # get only clusters that are unique\n",
    "unique_clusters = np.unique(cluster_list)\n",
    "unique_clusters = np.delete(unique_clusters, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bfe6c1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_csv_data_filter = combined_csv_data[combined_csv_data['cluster_id'].isin(unique_clusters)].drop_duplicates(subset=['cluster_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3e6b797",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale cluster_ids\n",
    "combined_csv_data_filter['label'] = combined_csv_data_filter.groupby('cluster_id').ngroup()\n",
    "#combine information with predictions and labels\n",
    "df_domain_f1 = pd.merge(final.drop(columns='sentence1'), combined_csv_data_filter.drop(columns=['cluster_id']), how=\"left\", on=[\"label\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8b9b972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score for Bikes: 1.0\n",
      "F1 Score for Cars: 0.8701298701298701\n",
      "F1 Score for Clothes: 0.7711978465679677\n",
      "F1 Score for Drugstore: 0.5866336633663366\n",
      "F1 Score for Electronics: 0.760904170646291\n",
      "F1 Score for Random: 0.8603326498063341\n",
      "F1 Score for Technology: 0.9616240266963292\n",
      "F1 Score for Tools: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bizer-tp2021/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1492: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(\n"
     ]
    }
   ],
   "source": [
    "for domain in domains:\n",
    "    f1_domain = f1_score(df_domain_f1[df_domain_f1['domain']==domain].label, df_domain_f1[df_domain_f1['domain']==domain].prediction, average='micro')\n",
    "    print(f\"F1 Score for {domain}: {f1_domain}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1db3aa",
   "metadata": {},
   "source": [
    "# F1 scores for size of train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5379d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the train set\n",
    "df_train = pd.read_csv(os.path.join(train_test_all_filtered_path_2,'df_train.csv'))\n",
    "#join label, predictions and amount test set\n",
    "df_amount_train_f1 = pd.merge(final.drop(columns='sentence1'), df_train.groupby('label').count(), how=\"left\", on=[\"label\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c632a3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [0, 5, 10, 25, 50, 100, 150]\n",
    "#create bins for categorization\n",
    "df_amount_train_f1['binned'] = pd.cut(df_amount_train_f1['sentence1'], bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "83cfe6bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score for size of train set per cluster in (10, 25]: 0.9335064935064935\n",
      "F1 Score for size of train set per cluster in (5, 10]: 0.8269733403031886\n",
      "F1 Score for size of train set per cluster in (0, 5]: 0.6513605442176871\n",
      "F1 Score for size of train set per cluster in (25, 50]: 0.9682352941176471\n",
      "F1 Score for size of train set per cluster in (50, 100]: 0.9788106630211895\n",
      "F1 Score for size of train set per cluster in (100, 150]: 0.9523809523809523\n"
     ]
    }
   ],
   "source": [
    "for interval in df_amount_train_f1['binned'].unique().to_list():\n",
    "    f1_train_size = f1_score(df_amount_train_f1[df_amount_train_f1['binned']==interval].label, df_amount_train_f1[df_amount_train_f1['binned']==interval].prediction, average='micro')\n",
    "    print(f\"F1 Score for size of train set per cluster in {interval}: {f1_train_size}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
