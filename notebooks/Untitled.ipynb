{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b68f530",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import fasttext\n",
    "import progressbar\n",
    "import json\n",
    "import gzip\n",
    "import shutil\n",
    "fasttext.FastText.eprint = lambda x: None # avoid Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a400205b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_parent = os.path.dirname(os.getcwd())\n",
    "pretrained_fasttext_path = os.path.join(path_parent, 'src/models/lid.176.bin')\n",
    "model = fasttext.load_model(pretrained_fasttext_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f154e21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This$ is a may2be an english 2text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98a7c745",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((), array([], dtype=float64))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(text, threshold = 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45c5e6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_with_fasttext():\n",
    "    \"\"\"\n",
    "    reads all files from cleaned data path and removes non-english products from the data tables\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    pretrained_fasttext_path = os.path.join(path_parent, 'src/models/lid.176.bin')\n",
    "    model = fasttext.load_model(pretrained_fasttext_path)\n",
    "\n",
    "    # list all top 100 product files\n",
    "    files = [file for file in os.listdir(cleaned_data_path) if file.endswith('.json.gz')]\n",
    "\n",
    "    removed_rows_dict = {}\n",
    "\n",
    "    count_files = 0\n",
    "\n",
    "    for file in files:\n",
    "        print(file)\n",
    "        df = pd.read_json(os.path.join(cleaned_data_path, '{}'.format(file)), compression='gzip', lines=True)\n",
    "        if df.shape[0] > 20: # for top100 & min3\n",
    "        #if df.shape[0] > 0: # for rest only\n",
    "            df['concat'] = ''\n",
    "\n",
    "            for j in range(df.shape[1]):  # iterate over columns\n",
    "                df['concat'] = df['concat'] + df.iloc[:,j].astype('str')\n",
    "\n",
    "            # iterrate over rows and save row_ids of english products\n",
    "            english_products = []\n",
    "            non_english_products = []\n",
    "            count = 0\n",
    "            with progressbar.ProgressBar(max_value=df.shape[0]) as bar:\n",
    "                for i in range(df.shape[0]):  # iterate over rows\n",
    "                    row_id = int(df['row_id'][i])\n",
    "                    cell = df['concat'][i]\n",
    "                    cell_pred = model.predict([cell])[0]\n",
    "                    print(cell_pred, model-predict([cell])[1])\n",
    "                    if cell_pred == [['__label__en']]:\n",
    "                        english_products.append(row_id)\n",
    "                    else:\n",
    "                        non_english_products.append(row_id)\n",
    "                    count += 1\n",
    "                    bar.update(count)\n",
    "\n",
    "            # drop concatendated row again\n",
    "            df = df.drop('concat', axis=1)\n",
    "\n",
    "            # write removed row_ids to dictionary\n",
    "            removed_rows_dict[file] = non_english_products\n",
    "\n",
    "            # design new dataframe with english products only\n",
    "            df_cleaned = df[df['row_id'].isin(english_products)]\n",
    "\n",
    "            # write to gzip compressed json file\n",
    "            if df_cleaned.shape[0] > 20: # for top100 & min3\n",
    "            #if df_cleaned.shape[0] > 0: # for rest only\n",
    "                df_cleaned.to_json(os.path.join(cleaned_data_path, '{}'.format(file)), compression='gzip', orient='records', lines=True)\n",
    "            else:\n",
    "                os.remove(os.path.join(cleaned_data_path, '{}'.format(file)))\n",
    "\n",
    "\n",
    "        else:\n",
    "            # if df does not contain more than 20 entries delete it from cleaned file path\n",
    "            os.remove(os.path.join(cleaned_data_path, '{}'.format(file)))\n",
    "\n",
    "        count_files += 1\n",
    "        print('{} out of {} files done'.format(count_files, len(files)))\n",
    "\n",
    "    # save dictionary with removed row_ids\n",
    "    with open(os.path.join(cleaned_data_path, 'removed_rows_dict.json'), 'w', encoding='utf-8') as f:\n",
    "        json.dump(removed_rows_dict, f)\n",
    "\n",
    "    print('removed_rows_dict saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ca1e156",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/work-ceph/bizer-tp2021/data_integration_using_deep_learning/src/data/product/LocalBusiness_top100/cleaned_new_threshold'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1677813/2106214808.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_parent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'src/data/product/LocalBusiness_top100'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcleaned_data_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cleaned_new_threshold'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mremove_with_fasttext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_1677813/957525141.py\u001b[0m in \u001b[0;36mremove_with_fasttext\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# list all top 100 product files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfile\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleaned_data_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.json.gz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mremoved_rows_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/work-ceph/bizer-tp2021/data_integration_using_deep_learning/src/data/product/LocalBusiness_top100/cleaned_new_threshold'"
     ]
    }
   ],
   "source": [
    "data_path = os.path.join(path_parent, 'src/data/product/LocalBusiness_top100')\n",
    "cleaned_data_path = os.path.join(data_path, 'cleaned_new_threshold')\n",
    "remove_with_fasttext()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
