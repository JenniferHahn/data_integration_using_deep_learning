<!DOCTYPE html>
<html>

<head>
    <title>Data Integration using Deep Learning</title>
    <link rel='stylesheet' href='http://webdatacommons.org/style.css' type='text/css' media='screen'/>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <style>
        .tar {
            text-align: right;
        }

        .rtable {
            float: right;
            padding-left: 10px;
        }

        .smalltable,
        .smalltable TD,
        .smalltable TH {
            font-size: 9pt;
        }

        .tab {
            overflow: hidden;
            border: 1px solid #ccc;
            background-color: #eaf3fa;
            clear: both;
            padding-left: 25px;
            width: 350px;
        }

        .tab button {
            background-color: inherit;
            float: left;
            border: none;
            outline: none;
            cursor: pointer;
            padding: 15px 60px;
            transition: 0.3s;
        }

        .tab button:hover {
            background-color: #ddd;
        }

        .tab button.active {
            background-color: #ccc;
        }

        .tabcontent {
            display: none;
            padding: 6px 12px;
            border-top: none;
            animation: fadeEffect 1s;
            width: 500px
        }

        .show {
            display: block;
        }

        .no-show {
            display: none;
        }

        caption {
            caption-side: top;
            font-style: italic;
        }

        td[scope="mergedcol"] {
            text-align: center;
        }

        hr {
            width: 50%;
            margin: 20px 0;
            /* This leaves 10px margin on left and right. If only right margin is needed try margin-right: 10px; */
        }

        @keyframes fadeEffect {
            from {
                opacity: 0;
            }

            to {
                opacity: 1;
            }
        }
    </style>
    <script type="text/javascript" src="https://www.google.com/jsapi"></script>
    <script type="text/javascript">
        google.load('visualization', '1', {
            packages: ['bar', 'line', 'corechart']
        });


    </script>

    <script type="text/javascript" src="http://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js"></script>
    <script type="text/javascript" src="../../jquery.toc.min.js"></script>
    <script type="text/javascript">
        var _gaq = _gaq || [];
        _gaq.push(['_setAccount', 'UA-30248817-1']);
        _gaq.push(['_trackPageview']);

        (function () {
            var ga = document.createElement('script');
            ga.type = 'text/javascript';
            ga.async = true;
            ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(ga, s);
        })();

        function openExpResult(evt, expName) {
            // Declare all variables
            var i, tabcontent, tablinks;

            // Get all elements with class="tabcontent" and hide them
            tabcontent = document.getElementsByClassName("tabcontent");
            for (i = 0; i < tabcontent.length; i++) {
                tabcontent[i].style.display = "none";
            }

            // Get all elements with class="tablinks" and remove the class "active"
            tablinks = document.getElementsByClassName("tablinks");
            for (i = 0; i < tablinks.length; i++) {
                tablinks[i].className = tablinks[i].className.replace(" active", "");
            }

            // Show the current tab, and add an "active" class to the button that opened the tab
            document.getElementById(expName).style.display = "block";
            evt.currentTarget.className += " active";
        }
    </script>

    <script type="application/ld+json">
        {
            "@context": "http://schema.org/",
            "@type": "Dataset",
            "name": "XXXXXXXX",
            "description": "XXXXXXXXXXX",
            "url": "XXXXXXX",
            "keywords": [
                "XXXXXX",
                "XXXXXX"
            ],
            "creator": [
                {
                    "@type": "Person",
                    "url": "XXXXXXX",
                    "name": "XXXXXXXX"
                },
                {
                    "@type": "Person",
                    "url": "XXXXXX",
                    "name": "XXXXXXX"
                }
            ],
            "citation": [
            ]
        }


    </script>

</head>

<body>
<div id="logo" style="text-align:right; background-color: white;">&nbsp;&nbsp;<a
        href="http://dws.informatik.uni-mannheim.de"><img src="images/ma-logo.gif"
                                                          alt="University of Mannheim - Logo"></a></div>
<div id="header">
    <h1 style="font-size: 250%;">Data Integration using Deep Learning</h1>
</div>
<div id="authors">
    <a>Christian Bizer</a></br>
    <a>Cheng Chen</a><br/>
    <a>Jennifer Hahn</a><br/>
    <a>Kim-Carolin Lindner</a></br>
    <a>Ralph Peeters</a></br>
    <a>Jannik Reißfelder</a><br/>
    <a>Marvin Rösel</a><br/>
    <a>Niklas Sabel</a><br/>
    <a>Luisa Theobald</a></br>
    <a>Estelle Weinstock</a></br>
</div>
<div id="content1">
    <p>
        Insert Task And Experiment Description/Abstract
        <br>
    </p>
    <p>
    The website is structured as follows. In chapter 1, we give a short introduction into the use cases and the challenges that
        are addressed by our project. Afterwards, we give an overview on the theoretical framework needed to keep track of the experiments.
        Chapter 3 establishes the algorithms we focused on followed by the creation and preparation of the task-specific datasets.
        Subsequently, we present our experiments including our baselines in chapter 5 in order to transition to  an error analysis
        in chapter 6. The website will be concluded by a discussion of the results and an outlook on further possibilities
        generated by our work.

    <h2>Contents</h2>
    <ul>
        <li class="toc-h2 toc-active">
            <a href="#toc1">1 Introduction</a>
        </li>
        <li class="toc-h2 toc-active">
            <a href="#toc2">2 Theoretical Framework</a>
            <ul>
                <li><a href="#toc2.1">2.1 Schema Matching</a></li>
                <li><a href="#toc2.2">2.2 Entity Matching</a></li>
                <li><a href="#toc2.3">2.3 Transformer Models</a></li>
            </ul>
        </li>
        <li class="toc-h2 toc-active">
            <a href="#toc3">3 Algorithms</a>
            <ul>
                <li><a href="#toc3.1">3.1 TURL</a></li>
                <li><a href="#toc3.3">3.2 Contrastive Learning</a></li>
            </ul>
        </li>
        <li class="toc-h2 toc-active">
            <a href="#toc4">4 Datasets and Preprocessing</a>
        </li>
        <li class="toc-h2 toc-active">
            <a href="#toc5">5 Experimental Setup</a>
            <ul>
                <li><a href="#toc5.1">5.1 Baseline Experiments</a></li>
                <li><a href="#toc5.2">5.2 Experiments</a></li>
            </ul>
        </li>
        <li class="toc-h2 toc-active">
            <a href="#toc6">6 Experimental Results and Error Analyis</a>
        </li>
        <li class="toc-h2 toc-active">
            <a href="#toc7">7 Discussion and Outlook</a>
        </li>
        <li class="toc-h2 toc-active">
            <a href="#toc8">8 References </a>
        </li>
        <li class="toc-h2 toc-active">
            <a href="#toc9">9 Downloads </a>
        </li>
    </ul>

    <span id="toc1"></span>
    <h2>1 Introduction</h2>

    <!--CONTENT-->
    <p>
        According to estimations of the International Data Corporation, the amount of data created in 2025 will be
        around 180 zettabytes with increasing tendency. Reasons for that are increasing connection and information flow
        due to the world wide web. The web contains a massive amount of data in all forms. There can be structured or
        unstructured data and a lot of different sources which use different data models or different schemata but
        actually describe the same real-world entity. Moreover, information can differ in content, syntax or even in
        technical ways.
        Therefore, it gets quite challenging when trying to use this heterogeneous data in order to compare and work with
        it for further applications such as online shopping to name just one example where data from different sources need
        to be compared.
        Addressing this problem, the aim of this work is to walk the above mentioned challenges and to establish
        different methods for both schema and entity matching.
    <p>
        All of our datasets can be downloaded in <a href="#toc9">section 9</a> but are available for research purposes only.
    <p>
        <i>Note:</i> The authors acknowledge support by the state of Baden-Württemberg through bwHPC.

        <!-- Outtakes-->
        <!-- and can be considered as a special type
        of <i>entity matching</i>.-->

        <span id="toc2"></span>
    <h2>2 Theoretical Framework </h2>
    <!--CONTENT-->
    <p>
        This chapter provides an overview of the theoretical underpinnings and specific information required for the traceability of our work. For this reason, we will
        introduce the main tasks we are trying to solve: Schema and Entity Matching. We also give a brief introduction to transformer models, especially BERT-based implementations,
        as they form the basis for the algorithms we use and became more and more popular in recent years,
        due to limitations of standard algorithms and measures.
    <p>
    <span id="toc2.1"></span>
    <h3>2.1 Schema Matching</h3>
    <p>
        Schema Matching describes the task of matching similar or rather the same schemata and finding agreement and unity between schemata.
        Database instances, for example, comprise of schemata and respective (table) columns describing the same attribute can be matched.
        The main challenges are large schemata, semantic heterogeneity, generic names, esoteric naming conventions and different languages.
        Therefore, correspondences between the schemes should be detected in an automated or semi-automated manner. Although 1:n and n:1 approaches are possible,
        the scope of the object is reduced and only 1:1 matching is considered as defined in the problem statement of the initial project discussion.
    <p>
    <span id="toc2.2"></span>
    <h3>2.2 Entity Matching</h3>
    <p>
        Entity matching, often also called identity resolution, is a crucial task for data integration and consists of the exercise of finding all records that refer to the same entity,
        e.g. when integrating data from different source systems. Unfortunately, entity representations in real-world
        environments are in general not identical or always complete, but have to be processed at massive scale. One solution to address this difficulty is offered by
        comparing multiple attributes of different record representations with attribute-specific similarity measures
        like Levenshtein distance or advanced techniques like BERT. Newer approaches include the application of
        so-called table transformers which will be discussed in <a href="#toc3">Section 3</a>. Entity matching tries to allocate entities
        with different representations under the assumption that the higher the similarity is, the more likely two
        entity representations are a match <a href="#toc8">[1]</a>.

    <span id="toc2.3"></span>
    <h3>2.3 Transformer Models</h3>
    <p>
        In 2017, Google Brain proposed the transformer model, that based on an encoder-decoder structure and an attention mechanism showed
        impressive improvements over state-of-the-art methods and simplicity in adoption to a wide range of machine learning tasks,
         especially in the context of NLP <a href="#toc8">[6]</a>.
        As a result, a new language representation model named BERT was introduced in 2019 to pre-train deep bidirectional
        representations from unlabeled text. That resulted in a lot of possibilities as a "pre-trained BERT model can be finetuned with
        just one additional output layer to create state-of-the art methods for a wide range of tasks" <a href="#toc8">[3]</a>. In the
        context of this work, we will use algorithms that are pre-trained on different BERT extensions, in particular TinyBERT and RoBERTa
        <a href="#toc8">[7]</a> <a href="#toc8">[8]</a>.
    <p>

    <span id="toc3"></span>
    <h2>3 Algorithms</h2>
    <p>
        In the following, we present different algorithms namely TURL from a class of table transformers and contrastive learning as
        <a style="color:#FF0000";>include CL...</a>. Table transformers are models that not only incorporate data from individual entries,
        but include information from their surroundings inside the table as well. This results in models which take a
        whole table representation of a website or a knowledge base as input instead of only single entries like most other models do.
        <a style="color:#FF0000";>include sth. for CL as well</a>
    </p>
    <span id="toc3.1"></span>
    <h3>3.1 TURL</h3>
    <p>
        Table Understanding through Representation Learning (TURL) is a "novel framework that introduces the pre-training/finetuning paradigm to
        relational Web tables" <a href="#toc8">[1]</a>. TURL is a TinyBERT based extension model that was pre-trained on around 600,000 Wikipedia tables such that it
         can be applied to different tasks with "minimal task-specific fine-tuning". The authors show that the model generalizes well and
         outperforms existing methods for example in column type annotation <a href="#toc8">[1]</a>.
        The basic idea of TURL is to "serialize[s] a table into a sequence of words and entities (similar to text data)
        and [to] learn[s] embedding vectors for words and entities using Word2Vec" <a href="#toc8">[1]</a>.
        More specifically, the TURL architecture which can be seen in Figure 3.1 consists of three modules.
       At first, an embedding layer followed by a structure-aware stacked transformer,as introduced in section <a href="#toc2.3">2.3</a>,
        to capture textual and relational knowledge with a "visibility matrix" that models the row-column structure
        of the tables and concluded by a projection layer.
    <figcaption style="margin-bottom:1em;">
        <strong>Figure 3.1: Overview TURL architecture
        </strong>
    </figcaption>
    <figure style="margin-top:3em">
        <img src="./images/TURL framework.JPG" style="margin-bottom:2em;"/>
    </figure>

    <p>
        After the described pre-training procedure the model can then be applied to different proposed fine-tuning tasks
        such as entity linking, column type annotation, relation extraction, row population, cell filling, and schema
        augmentation.
    </p>
    </p>

    <span id="toc3.2"></span>
    <h3>3.2 Contrastive Learning</h3>
    <p>
        Contrastive learning has become a promising approach both in information retrieval <a href="#toc8">[7]</a> as well as in computer vision outperforming previous methods
        in self-supervised and semi-supervised learning <a href="#toc8">[6]</a>. This framework has further been extended to a fully-supervised setting introducing an alternative
        to the usual cross-entropy loss function <a href="#toc8">[5]</a> while achieving state-of-the-art results. In addition, supervised contrastive learning has seen recent success
        in product matching which is a special form of entity matching <a href="#toc8">[4]</a>.

        <figcaption style="margin-bottom:1em;">
            <strong>Figure 3.2: Supervised Contrastive Learning
            </strong>
        </figcaption>
        <figure style="margin-top:3em">
            <img src="./images/supervised contrastive loss.JPG" style="margin-bottom:2em;"/>
        </figure>

        Figure 3.2 summarizes the general framework of supervised contrastive learning. In general, the whole process is split up in two stages, pretraining
        followed by the second stage of finetuning. The main purpose of contrastive pretraining is to learn hidden representations of respective clusters in such a way
        that instances from the same class end up close in space while instances from different classes end up far in space.
        To this end, the pretraining usually requires pairs of training instances: a positive is defined as a pair of training instances drawn
        from the same class, while negatives are training instances drawn from different classes. At training time these pairs are fed through two distinct encoder networks which share the same weights.
        This yields two latent representations which can now be used to compute the supervised contrastive (SupCon) loss which usually involves some sort of similarity metric. The SupCon loss function then
        maximizes the agreement between positives while it minimizes the agreement between negatives.
    <p>
        After the pretraining step, the encoder network ideally maps each class to a well seperated cluster in the embedding space. For the final stage of finetuning
        the parameters of the encoder network remain frozen and only the linear classifier is trained using the usal cross-entropy loss for multi-class classification.
        In contrast to other methods, contrastive pretraining makes it much easier to learn decision boundaries for a linear classifier given a pretrained embedding space.
        This makes contrastive pretraining a powerful method for further downstream tasks such as multi-class classification.

    </p>


    </p>
    <span id="toc4"></span>
    <h2>4 Datasets and Preprocessing </h2>
    <!--CONTENT-->
    <p>
        <!-- Concept Explanation-->
        Chapter 4 gives an overview on the generation of our datasets and the final values contained in the sets.
        Our dataset is based on the Web Data Commons - Schema.org Table Corpus <a href="#toc8">[10]</a> maintained by the Data
        and Web Science Research Group at the University of Mannheim.
        <br> For both tasks, i.e. entity and schema matching, different selection and filtering methods were applied to attain the final data sets.
        However, in a first action, chosen tables for both taks were cleaned using a two-step approach in order extract English language data only.
        We applied a TLD-based method to first filter our data on English internet domain
        endings, e.g. ".com",or ".uk", and afterwards applied the fastText language detection algorithm on each single row in
        the remaining tables to check whether it belongs to the English language and if not we discarded them  <a href="#toc8">[9]</a><.
        Further cleaning of selected tables and further preprocessing approaches specific to each task of schema or entity matching will be explained in detail in the following.
        <br>
        <a style="color:#FF0000";>
            - I know this applies to both/all groups but this mixes up the order a bit since we first selected
            specific classes, tables etc.?
            <br>
            - Kim: Is it important to know here from how much data we started and to which number of data we end up after cleaning it in the first step? Should we include that?
            <br>
            - Niklas: since they want us to focus on statistics, I would say yes
            <br>
            - Jen: Hatte die anzahl und das herunterbrechen in meinem Schema Teil relativ detailliert erklärt, also würde sagen ja - hab das english cleaning bisschen umformuliert, aber können das sonst auch bei euch
            integrieren und ich schreibe bei mir darunter, dass das genauso applied wurde?
        </a>

      <span id="toc4.1"></span>
      <h3>4.1 Entity Matching</h3><br>
        Within entity matching we focused on a specific part of the corpus, namely the Product data as biggest entity type
        with 1.662 million tables and 231 million entities in the parts Top100 and Minimum3, while excluding the tables in the Rest part,
        because those tables do not have enough entries. After our initial language cleaning step, we remained with 435,000 tables and  <a style="color:#FF0000";>xx </a> entities.
        The Product corpus already provided a clustering for the entities, so, no
        further annotation was needed. Here, a cluster corresponds to a collection of identical products across different origins, i.e. websites.
        Focusing on clusters with at least eight tables, we used the most common brands for different
        chosen categories, in particular bikes, cars, clothes, drugstore, electronics,
        and technology as keywords to get relevant clusters for our final dataset. To further enhance our data, we
        searched for brands with at least 1,000 distinct clusters and established another category called "random". In order to
        make the final matching for the algorithms not too easy, we browsed the selected clusters for homogeneous entities
        using Doc2Vec <a href="#toc8">[11]</a> and Jaccard similarity to include hard-to-distinguish clusters in our dataset.
        Here, we based our selection on the balance between both, Jaccard and Doc2Vec score, by manually validating the best
        results for each of the defined domains.
        <p>
            To get a better understanding of hard-to-distinguish cases, we provide some examples in Table 4.1.
            The upper table illustrates three examples of so-called hard non-matching cases. A hard non-match describes two entities which are semantically very close
            but belong to different classes.  As one can see, some entities only differ in some characters, resulting in different cluster assignments.
            On the other hand, hard matches relate to two entities which belong to the same cluster but are semantically not as close to entities of the same class.
            This can easily happen when the same entities differ in the character length of their respective name column.
            For hard non-matches we provide the corresponding cosine similarity between a query entity and its closest match while for hard matches we show
            the cosine metric between the query and the closest product in the same cluster. In both cases, the cosine similarity is computed by comparing the vectorized
            entities obtained by Doc2Vec.
        </p>  
        
        <table style="width:100%">
            <caption style="margin-top: 1em">Table 4.1: Illustration of hard non-matches and hard matches .</caption>
            <thead>
              <tr>
                <th>Entity</th>
                <th>Most Similar Entity</th>
                <th>Cosine Metric</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>Lifeproof Case Iphone 11</td>
                <td>iPhone 11 <b>Pro Max</b> case</td>
                <td>0.9776</td>
              </tr>
              <tr>
                <td>Lego Star Wars The Complete Saga <b>DS</b></td>
                <td>Lego Star Wars: The Complete Saga - <b>Wii Video</b> Game</td>
                <td>0.9367</td>
              </tr>
              <tr>
                <td> <b>10 2010</b> Audi A5 Quattro Fuel Injector 2.0L 4 Cyl Bosch High Pressure</td>
                <td> <b>18 2018</b> Audi A5 Quattro Fuel Injector 2.0L 4 Cyl Standard Motor Products</td>
                <td>0.9771</td>
              </tr>
            </tbody>
            <thead>
                <tr>
                  <th>Entity</th>
                  <th>Matching Entity</th>
                  <th>Cosine Metric</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>iPhone 11 <b>Pro Max</b> case</td>
                  <td>For iphone 11 <b>pro</b> x xr xs <b>max</b> cell phone case cover with camera lens protection</td>
                  <td>0.8514 (below top 30)</td>
                </tr>
                <tr>
                  <td>08MP-08FPS 90° Elbow <b>Long</b> Forged</td>
                  <td>08MP-08FPS 90° Elbow Forged</td>
                  <td>0.9062 (15th place)</td>
                </tr>
                <tr>
                  <td>Jasmine Dragon Pearls Green Tea</td>
                  <td>Jasmine Dragon Pearl <b>Jasmine</b> Green Tea</td>
                  <td>0.9501 (5th place)</td>
                </tr>
              </tbody>
            </table>
        
        <a style="color:#FF0000";>(@Estelle postpreprocessing der gesamten tabellen einfügen)</a>
        With a multilabel stratified shuffle-split <a style="color:#FF0000";>(@Kim/Jen: Ref? https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html or just cite sklearn?)</a> we distributed the remaining clusters
        into &frac38; train, &frac14; validation and &frac38; test data. With this approach we ensured that
        tables were distributed equally across size and selected clusters. Furthermore, in favor of not distorting the results
        we cleaned the test set manually and discarded around 10 % of noise. Thereby, we ended up with an amount of
        1,410 clusters that were used for training the algorithms. The final set sizes can be seen in Table 4.1.


    </p>
    <table style="width:20%">
        <caption style="margin-top: 1em">Table 4.2: Final product dataset sizes for entity matching.</caption>
        <thead>
          <tr>
            <th></th>
            <th>Number of Tables</th>
            <th>Number of Entities</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Train</td>
            <td>1,345</td>
            <td>11,121</td>
          </tr>
          <tr>
            <td>Validation</td>
            <td>885</td>
            <td>7,154</td>
          </tr>
          <tr>
            <td>Test</td>
            <td>1,331</td>
            <td>10,655</td>
          </tr>
        </tbody>
        </table>

        <span id="toc4.2"></span>
        <h3>4.2 Schema Matching</h3>
        <p>
            As a basis for schema matching, it was important to gather a large amount of diverse table data to create a sufficient dataset
            with at least 200 different columns. Thereby, the chosen columns should be an evenly distributed within large, mid-sized and small tables,
            and their content should represent different data types, different lengths and as well as hard cases for distinction.
            To create a solid data base, we choose the 20 largest categories of Schema.org in order to gain a large amount of
            tables that contain a sufficient amount of data. Due to the vast amount of disorderly data we initally chose 208 columns to be able
            to further reduce the column set in case that we would detect further misfitting criteria during the following data preparation and preprocessing.
            However, as we wanted to make sure that all tables contain at least three of the selected columns for schema matching, our initial dataset contained 79,318 tables.
            Further, in order to create a useful dataset, we reduced our sample of od tables further by checking for tables with more than 10 rows,
            less than 50% NAs within selected columns​ as well as less than 15% NAs within relevant columns in entire table.
            <br> The resulting 54,190 tables were then split into
            a training and set set by performing a multi-label stratified shuffled split with 3 splits (random state = 42) resulting in 44,435 training tables and 9,894 test tables.
            We chose to perform a <a
                href="https://github.com/trent-b/iterative-stratification">multi-label stratified shuffled split</a> to not only distribute the different categories
            but also the selected columns in each category proportionally between the training and test set.
            To compare different input and training sizes later on, our training data was further divided into a medium as well as small sized training set. Again,
            we performed a multi-label stratified shuffled split to distribute the data and especially columns proportinally making sure that all selected columns
            were represented in all datasets. Hereby, the large training set of 44,345 tables contaings all 9,776 tables of the medium-sized training and the medium-sized training
            set contains all 2,444 tables of the small training set.
            <br>
            As a means to create a clean and reliable set of test tables, the entire test set containing 9,894 tables was manually checked for languages other than English, wrong column
            labels as well as multiple, missing or odd entries such as symbols, for example. Thereby, about 10 percent of the tables were removed with more than 50 percent due to other foreign languages,
            hence reducing the test table size to 8,912.
            <br>
              <a style="color:#FF0000";>(@Estelle: Könntest du hier nochmal beispiele zu den hard cases (irgendwas, was du schon mal in der präsi damals hattest?) integrieren?)</a>

            The distribution of all tables as well as tables within each selected category is presented in the table below.

        </p>
        <table>
            <caption style="margin-top: 1em">Table 4.2: Final data set sizes and category distribution for schema matching.</caption>
            <thead>
              <tr>
                <th></th>
                <th>Small Training Set</th>
                <th>Medium Training Set</th>
                <th>Large Training Set</th>
                <th>Test Set</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>All</td>
                <td>2,444</td>
                <td>9,776</td>
                <td>44,345</td>
                <td>8,912</td>
              </tr>
              <tr>
                <td>Product</td>
                <td>1,033</td>
                <td>4,256</td>
                <td>19,367</td>
                <td>3,839</td>
              </tr>
              <tr>
                <td>Music Recording</td>
                <td>318</td>
                <td>1,102</td>
                <td>5,031</td>
                <td>1,097</td>
              </tr>
              <tr>
                <td>Event</td>
                <td>248</td>
                <td>1,007</td>
                <td>4,563</td>
                <td>1,000</td>
              </tr>
                <td>Creative Work</td>
                <td>221</td>
                <td>869</td>
                <td>3,925</td>
                <td>876</td>
              </tr>
                <td>Recipe</td>
                <td>195</td>
                <td>770</td>
                <td>3,522</td>
                <td>727</td>
              </tr>
              </tr>
                <td>Person</td>
                <td>163</td>
                <td>690</td>
                <td>3,148</td>
                <td>545</td>
              </tr>
              </tr>
                <td>Local Business</td>
                <td>123</td>
                <td>490</td>
                <td>2,209</td>
                <td>381</td>
              </tr>
              </tr>
                <td>Place</td>
                <td>38</td>
                <td>160</td>
                <td>728</td>
                <td>131</td>
              </tr>
              </tr>
                <td>Hotel</td>
                <td>38</td>
                <td>156</td>
                <td>701</td>
                <td>117</td>
              </tr>
              </tr>
                <td>Book</td>
                <td>29</td>
                <td>118</td>
                <td>537</td>
                <td>65</td>
              </tr>
              </tr>
                <td>Restaurant</td>
                <td>20</td>
                <td>79</td>
                <td>353</td>
                <td>61</td>
              </tr>
              </tr>
                <td>Music Album</td>
                <td>9</td>
                <td>41</td>
                <td>189</td>
                <td>42</td>
              </tr>
              </tr>
                <td>TV Episode</td>
                <td>9</td>
                <td>38</td>
                <td>162</td>
                <td>31</td>
              </tr>
            </tbody>
            </table>


<p>
    All of our datasets can be downloaded in <a href="#toc9">section 9</a> but are available for research purposes only.

</p>
<span id="toc5"></span>
<h2>5 Experimental Setup</h2>
<p>
    For running the experiments we used the resources from the University of Mannheim(dws-server) as well as the resources from the state Baden-Württemberg,
    the bw-uni-cluster. With that we had access to different
     setups to efficiently run the experiments. Furthermore, we had enough storage space to store
    the different datasets, created for the experiments.
</p>
<span id="toc5.1"></span>
<h3>5.1 Baseline Experiments</h3>
<!--CONTENT-->
<p>
    In the case of entity matching, we use three different baseline models to be able to compare the results of our algorithms.
    We include one tree-based model in Random Forest, and two BERT-based models in TinyBERT and RoBERTa, because, as mentioned
    in section \ref{cha:alg}, TURL is based on TinyBERT and Tabbie on BERT.
    All the baselines were modelled as multi-class classification that were presented a concatentation of the entity name and
    a description in case of the product dataset respectively the name of the  entity in case of the LocalBusiness dataset.
    The results are presented in Table <a style="color:#FF0000";>(x.x)</a>.
    <table style="width:25%">
        <caption style="margin-top: 1em">Table 4.2: F1-scores for different baselines models in entity matching.</caption>
        <thead>
          <tr>
            <th></th>
            <th>Random Forest</th>
            <th>TinyBert</th>
            <th>RoBERTa</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Product</td>
            <td>0.8684</td>
            <td>0.8329</td>
            <td>0.8958</td>
          </tr>
        </tbody>
        </table>
<p>
    As a baseline for schema matching, both tree and bert based models are used. The tree based models use a random forest with
    both value and meta based data. The value based data sets are both created with TF-IDF. Hereby, a global and a binary approach is used.
    For TF-IDF the data is preprocessed with the following steps: the concatenated text of each column is lower-cased and tokenized. Also,
    stopwords and punctuation is removed.
    The meta approach is based on the following properties of the data: whether or not it has a structure such as {},
    the length of the value, the average word length and a binary variable whether the column includes dates. To keep the original
    structure of the data, no preprocessing is done. Hereby, the regular TF-IDF approach yields a micro F1-Score of 0.35 and the binary TF-IDF
    approach yields a micro F1-Score of 0.27 while the meta approach yields a micro F1-Score of 0.12.
    Further, we used bert based models such as Bert, RoBERTa, TinyBert and Distilbert as baseline models for the
    respective small, medium as well as large training data set. As a data base the concatenated values of the target columns were used.
    To keep as much information as possible, no further preprocessing was done. The models were trained with 25 epochs. As can be
    seen in the results in table 4.3, the models perform quite different on the small and medium datasets. Except for TinyBert,
    the performance of the different models converges for the large data set.

    <table style="width:25%">
        <caption style="margin-top: 1em">Table 4.3: Micro F1-scores for bert-based baseline models in schema matching.</caption>
            <thead>
              <tr>
                <th></th>
                <th>Distilbert</th>
                <th>Bert</th>
                <th>TinyBert</th>
                <th>RoBERTa</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>Small</td>
                <td>0.60</td>
                <td>0.73</td>
                <td>0.69</td>
                <td>0.66</td>
              </tr>
              <tr>
                <td>Medium</td>
                <td>0.62</td>
                <td>0.76</td>
                <td>0.70</td>
                <td>0.77</td>
              </tr>
              <tr>
                <td>Large</td>
                <td>0.80</td>
                <td>0.80</td>
                <td>0.76</td>
                <td>0.80</td>
              </tr>
            </tbody>
            </table>

<p>
    As mentioned earlier, TURL is based on TinyBert so that this represents a feasable baseline model and is mainly used
    for comparison and evaluation of the results later on.
</p>

    <!-- include pair-wise trainsizes?-->

    <span id="toc5.2"></span>
<h3>5.2 TURL Experiments</h3>
<!--CONTENT-->
<p>

    All of our experiments were conducted using <a
        href="https://huggingface.co/transformers/main_classes/trainer.html">HuggingFace's Trainer</a> wrapper class for PyTorch. <br>

    TURL does already offer several predefined tasks to evaluate the pre-trained framework on <a href="#toc8">[1]</a>.
    Hereby, the taks of column type annotation is most suitable for both entity as well as schema matching. In order to pretain the model with our selected tables and
    entities and perform the task of column type annotation the data has to be structured as a list containing lists for each table. Each table list then contains the
    table id, the page title, section title as well as further lists of headers, cell content and the column types.
    The given input representation of tables for the task of column type annotation can be found within the <a
        href="https://github.com/sunlab-osu/TURL#readme"> README</a> file within the TURL <a
            href="https://github.com/sunlab-osu/TURL"> git repository</a>.
    Further information on the pretraining and the respective fine-tuning task can be found in TURL: Table Understanding through Representation Learning <a href="#toc8">[1]</a>.
    <br>
    Detailed information on settings and hyperparameters adjusted during the experimentation are presented in the table below.


</p>
<br>
<div style="margin-top:2em;">
    <div style=";margin-right: 2.5em">
        <div class="tab">
            <button class="tablinks" onclick="openExpResult(event, 'schema')">Schema</button>
            <button class="tablinks" onclick="openExpResult(event, 'entity')">Entity</button>
        </div>
        <div id="schema" class="tabcontent" style="display: none;">
            <table class="Multi-class">
                <caption style="margin-top: 1em">Table 3: Experimental results</caption>
                <tr>
                    <th>Category</th>
                    <th colspan="1" style='text-align:center; vertical-align:middle'> Inital</th>
                    <th colspan="1" style='text-align:center; vertical-align:middle'> Final/Last</th>
                </tr>

                <tr>
                    <td>Training Epochs</td>
                    <td>10</td>
                    <td>DE</td>
                    <td style="background-color:#E8E8E8">SVM</td>
                    <td style="background-color:#E8E8E8">TF-IDF</td>
                    <td style="background-color:#E8E8E8">0.9677</td>
                    <td>0.9668<i><sub>t</sub></i></td>
                    <td><b>0.9745</b></td>
                    <td>0.9626</td>

                </tr>
                <tr>
                    <td>Learning Rate</td>
                    <td>5e-5</td>
                    <td style="background-color:#E8E8E8">SVM</td>
                    <td style="background-color:#E8E8E8">TF-IDF</td>
                    <td style="background-color:#E8E8E8"><b>0.7084</b></td>
                    <td>0.4713</td>
                    <td>0.6176</td>
                    <td>0.5955<i><sub>t</sub></i></td>
                </tr>
                <tr>
                    <td>Batch Size</td>
                    <td>20</td>
                    <td style="background-color:#E8E8E8">Logistic Regression</td>
                    <td style="background-color:#E8E8E8">TF-IDF</td>
                    <td style="background-color:#E8E8E8">0.9683</td>
                    <td><b>0.9836</b></td>
                    <td>0.9754</td>
                    <td>0.9743</td>
                </tr>
                <tr>
                    <td>Accumulation Steps</td>
                    <td>2</td>
                    <td style="background-color:#E8E8E8">Logistic Regression</td>
                    <td style="background-color:#E8E8E8">TF-IDF</td>
                    <td style="background-color:#E8E8E8"><b>0.6735<i><sub>t</sub></i></b></td>
                    <td>0.3722</td>
                    <td>0.5619</td>
                    <td>0.4812</td>
                </tr>
                <tr>
                    <td>Save Steps</td>
                    <td>5000</td>
                    <td style="background-color:#E8E8E8">Logistic Regression</td>
                    <td style="background-color:#E8E8E8">TF-IDF</td>
                    <td style="background-color:#E8E8E8"><b>0.7047</b></td>
                    <td>0.4826</td>
                    <td>0.5975</td>
                    <td>0.5658<i><sub>t</sub></i></td>
                </tr>
                <tr>
                    <td>Logging Steps</td>
                    <td>1500</td>
                    <td style="background-color:#E8E8E8">SVM</td>
                    <td style="background-color:#E8E8E8">TF-IDF</td>
                    <td style="background-color:#E8E8E8">0.9757</td>
                    <td>0.9789</td>
                    <td><b>0.9828</b></td>
                    <td>0.9723</td>
                </tr>
                <tr>
                    <td>Warm up Steps</td>
                    <td>5000</td>
                    <td style="background-color:#E8E8E8">-</td>
                    <td style="background-color:#E8E8E8">-</td>
                    <td style="background-color:#E8E8E8">-</td>
                    <td>0.9802</td>
                    <td><b>0.9823</b></td>
                    <td>0.977</td>
                </tr>
                <tr>
                    <td>EN+DE</td>
                    <td>ES</td>
                    <td style="background-color:#E8E8E8">-</td>
                    <td style="background-color:#E8E8E8">-</td>
                    <td style="background-color:#E8E8E8">-</td>
                    <td>0.5034</td>
                    <td><b>0.6602<i><sub>t</sub></i></b></td>
                    <td>0.5792</td>
                </tr>
                <tr>
                    <td>EN+DE</td>
                    <td>FR</td>
                    <td style="background-color:#E8E8E8">-</td>
                    <td style="background-color:#E8E8E8">-</td>
                    <td style="background-color:#E8E8E8">-</td>
                    <td>0.563</td>
                    <td><b>0.6695</b></td>
                    <td>0.6384<i><sub>t</sub></i></td>
                </tr>
            </table>
        </div>
        <div id="entity" class="tabcontent" style="display: none;">
            <table class="pair-wiseSmall">
                <caption style="margin-top: 1em">Table 3: Experimental results</caption>
                <tr>
                    <th>Category</th>
                    <th colspan="2" style='text-align:center; vertical-align:middle'> Setting</th>
                    <th colspan="3" style='text-align:center; vertical-align:middle'> Baseline</th>
                    <th colspan="3" style='text-align:center; vertical-align:middle'> Transformer</th>
                </tr>
                <tr>
                    <th></th>
                    <th>Train Lang</th>
                    <th>Test Lang</th>
                    <th>Classifier</th>
                    <th>Embedding</th>
                    <th>Score</th>
                    <th>BERT</th>
                    <th>mBERT</th>
                    <th>XLM-R</th>
                </tr>
                <tr>
                    <td rowspan="9" style='text-align:center; vertical-align:middle'>Toy</td>
                    <td>DE</td>
                    <td>DE</td>
                    <td style="background-color:#E8E8E8">SVM</td>
                    <td style="background-color:#E8E8E8">Co-Occurence</td>
                    <td style="background-color:#E8E8E8">0.8298</td>
                    <td>0.8984</td>
                    <td><b>0.948</b></td>
                    <td>0.9026</td>
                </tr>
                <tr>
                    <td>EN</td>
                    <td>DE</td>
                    <td style="background-color:#E8E8E8">Random Forest</td>
                    <td style="background-color:#E8E8E8">Co-Occurence</td>
                    <td style="background-color:#E8E8E8">0.6746<i><sub>t</sub></i></td>
                    <td>0.6777<i><sub>t</sub></i></td>
                    <td><b>0.8891</b></td>
                    <td>0.882</td>
                </tr>
                <tr>
                    <td>EN</td>
                    <td>EN</td>
                    <td style="background-color:#E8E8E8">Logistic Regression</td>
                    <td style="background-color:#E8E8E8">Co-Occurence</td>
                    <td style="background-color:#E8E8E8">0.8263</td>
                    <td>0.9167</td>
                    <td>0.9223</td>
                    <td><b>0.9284</b></td>
                </tr>
                <tr>
                    <td>EN</td>
                    <td>ES</td>
                    <td style="background-color:#E8E8E8">Logistic Regression</td>
                    <td style="background-color:#E8E8E8">Co-Occurence</td>
                    <td style="background-color:#E8E8E8">0.625<i><sub>t</sub></i></td>
                    <td>0.6767<i><sub>t</sub></i></td>
                    <td><b>0.8043</b></td>
                    <td>0.7725</td>
                </tr>
                <tr>
                    <td>EN</td>
                    <td>FR</td>
                    <td style="background-color:#E8E8E8">SVM</td>
                    <td style="background-color:#E8E8E8">Co-Occurence</td>
                    <td style="background-color:#E8E8E8">0.6732</td>
                    <td>0.7416</td>
                    <td><b>0.8002</b></td>
                    <td>0.7748</td>
                </tr>
                <tr>
                    <td>EN+DE</td>
                    <td>DE</td>
                    <td style="background-color:#E8E8E8">Logistic Regression</td>
                    <td style="background-color:#E8E8E8">Co-Occurence</td>
                    <td style="background-color:#E8E8E8">0.8146</td>
                    <td>0.9236</td>
                    <td>0.9397</td>
                    <td><b>0.9401</b></td>
                </tr>
                <tr>
                    <td>EN+DE</td>
                    <td>EN</td>
                    <td style="background-color:#E8E8E8">-</td>
                    <td style="background-color:#E8E8E8">-</td>
                    <td style="background-color:#E8E8E8">-</td>
                    <td>0.9258</td>
                    <td>0.9239</td>
                    <td><b>0.9275</b></td>
                </tr>
                <tr>
                    <td>EN+DE</td>
                    <td>ES</td>
                    <td style="background-color:#E8E8E8">-</td>
                    <td style="background-color:#E8E8E8">-</td>
                    <td style="background-color:#E8E8E8">-</td>
                    <td>0.7528</td>
                    <td><b>0.8467</b></td>
                    <td>0.8388</td>
                </tr>
                <tr>
                    <td>EN+DE</td>
                    <td>FR</td>
                    <td style="background-color:#E8E8E8">-</td>
                    <td style="background-color:#E8E8E8">-</td>
                    <td style="background-color:#E8E8E8">-</td>
                    <td>0.75</td>
                    <td>0.8298</td>
                    <td><b>0.8299</b></td>
                </tr>
            </table>
        </div>
    </div>
</div>


<span id="toc5.3"></span>
<h3> 5.3 Contrastive Learning Experiments </h3>
<p>
    All our experiments presented in this section build upon the previous work on supervised contrastive learning for product matching <a href="#toc8">[4]</a>
    and the authors' <a href="https://github.com/wbsg-uni-mannheim/contrastive-product-matching">repository</a>. Their model already offered the use-case of binary pair-wise classification for product matching.
    From there we reformulated their approach to serve as a multi-class classification framework, for both schema and entitiy matching.
    As this approach is not table-based anymore, we had to restructure our data in a similar way as we did for our baseline experiments. While for product matching we only had one
    dataset available we conducted experiments on all three datasets for schema matching namely small, medium and large. Furthermore, for product we concatenated
    the name and description columns and appended the respective cluster id as the label column. For schema matching the whole column information is concatenated into
    one text column followed the corresponding label. In both cases we basically got rid of the table content information as all entries are compressed into the same file.
    For contrastive pretraining we then combined training and validation sets into a single one to rely on more data. As mentioned before, the whole power of this apporach builds on
    meaningful contrastive pretraining. It is for the same reason that for product, we decided for a large batch size to avoid noisy gradient signals. In the case of schema matching
    we were not able to set batch size as high as desired due to resource contraints. This is important to note and will be discussed later.
    After successful pretraining we froze the parameters of the encoder network and only trained a linear classifier which is a basic feed forward neural network.
    Detailed information on the respective hyperparameter settings during experimentation are presented in the table below.


</p>
<br>
<div style="margin-top:2em;">
    <div style=";margin-right: 2.5em">
        <div class="tab">
            <button class="tablinks" onclick="openExpResult(event, 'schemacon')">Schema</button>
            <button class="tablinks" onclick="openExpResult(event, 'entitycon')">Entity</button>
        </div>
        <div id="schemacon" class="tabcontent" style="display: none;">
            <table class="Multi-class">
                <caption style="margin-top: 1em">Table 3: Experimental results</caption>
                <tr>
                    <th>Category</th>
                    <th colspan="1" style='text-align:center; vertical-align:middle'> Inital</th>
                    <th colspan="1" style='text-align:center; vertical-align:middle'> Final/Last</th>
                </tr>
                <tr>
                    <th></th>
                    <th>Epochs</th>
                    <th>Learning Rate</th>
                    <th>Step Sizes </th>
                    <th>...</th>
                    <th>...</th>
                    <th>BERT</th>
                    <th>mBERT</th>
                    <th>XLM-R</th>

                </tr>
                <tr>
                    <td rowspan="9" style='text-align:center; vertical-align:middle'>Toy</td>
                    <td>DE</td>
                    <td>DE</td>
                    <td style="background-color:#E8E8E8">SVM</td>
                    <td style="background-color:#E8E8E8">TF-IDF</td>
                    <td style="background-color:#E8E8E8">0.9677</td>
                    <td>0.9668<i><sub>t</sub></i></td>
                    <td><b>0.9745</b></td>
                    <td>0.9626</td>

                </tr>
                <tr>
                    <td>EN</td>
                    <td>DE</td>
                    <td style="background-color:#E8E8E8">SVM</td>
                    <td style="background-color:#E8E8E8">TF-IDF</td>
                    <td style="background-color:#E8E8E8"><b>0.7084</b></td>
                    <td>0.4713</td>
                    <td>0.6176</td>
                    <td>0.5955<i><sub>t</sub></i></td>
                </tr>
                <tr>
                    <td>EN</td>
                    <td>EN</td>
                    <td style="background-color:#E8E8E8">Logistic Regression</td>
                    <td style="background-color:#E8E8E8">TF-IDF</td>
                    <td style="background-color:#E8E8E8">0.9683</td>
                    <td><b>0.9836</b></td>
                    <td>0.9754</td>
                    <td>0.9743</td>
                </tr>
                <tr>
                    <td>EN</td>
                    <td>ES</td>
                    <td style="background-color:#E8E8E8">Logistic Regression</td>
                    <td style="background-color:#E8E8E8">TF-IDF</td>
                    <td style="background-color:#E8E8E8"><b>0.6735<i><sub>t</sub></i></b></td>
                    <td>0.3722</td>
                    <td>0.5619</td>
                    <td>0.4812</td>
                </tr>
                <tr>
                    <td>EN</td>
                    <td>FR</td>
                    <td style="background-color:#E8E8E8">Logistic Regression</td>
                    <td style="background-color:#E8E8E8">TF-IDF</td>
                    <td style="background-color:#E8E8E8"><b>0.7047</b></td>
                    <td>0.4826</td>
                    <td>0.5975</td>
                    <td>0.5658<i><sub>t</sub></i></td>
                </tr>
                <tr>
                    <td>EN+DE</td>
                    <td>DE</td>
                    <td style="background-color:#E8E8E8">SVM</td>
                    <td style="background-color:#E8E8E8">TF-IDF</td>
                    <td style="background-color:#E8E8E8">0.9757</td>
                    <td>0.9789</td>
                    <td><b>0.9828</b></td>
                    <td>0.9723</td>
                </tr>
                <tr>
                    <td>EN+DE</td>
                    <td>EN</td>
                    <td style="background-color:#E8E8E8">-</td>
                    <td style="background-color:#E8E8E8">-</td>
                    <td style="background-color:#E8E8E8">-</td>
                    <td>0.9802</td>
                    <td><b>0.9823</b></td>
                    <td>0.977</td>
                </tr>
                <tr>
                    <td>EN+DE</td>
                    <td>ES</td>
                    <td style="background-color:#E8E8E8">-</td>
                    <td style="background-color:#E8E8E8">-</td>
                    <td style="background-color:#E8E8E8">-</td>
                    <td>0.5034</td>
                    <td><b>0.6602<i><sub>t</sub></i></b></td>
                    <td>0.5792</td>
                </tr>
                <tr>
                    <td>EN+DE</td>
                    <td>FR</td>
                    <td style="background-color:#E8E8E8">-</td>
                    <td style="background-color:#E8E8E8">-</td>
                    <td style="background-color:#E8E8E8">-</td>
                    <td>0.563</td>
                    <td><b>0.6695</b></td>
                    <td>0.6384<i><sub>t</sub></i></td>
                </tr>
            </table>
        </div>
        <div id="entitycon" class="tabcontent" style="display: none;">
            <table class="pair-wiseSmall">
                <caption style="margin-top: 1em">Table 3: Experimental results</caption>
                <tr>
                    <th>Category</th>
                    <th colspan="2" style='text-align:center; vertical-align:middle'> Setting</th>
                    <th colspan="3" style='text-align:center; vertical-align:middle'> Baseline</th>
                    <th colspan="3" style='text-align:center; vertical-align:middle'> Transformer</th>
                </tr>
                <tr>
                    <th></th>
                    <th>Train Lang</th>
                    <th>Test Lang</th>
                    <th>Classifier</th>
                    <th>Embedding</th>
                    <th>Score</th>
                    <th>BERT</th>
                    <th>mBERT</th>
                    <th>XLM-R</th>
                </tr>
                <tr>
                    <td rowspan="9" style='text-align:center; vertical-align:middle'>Toy</td>
                    <td>DE</td>
                    <td>DE</td>
                    <td style="background-color:#E8E8E8">SVM</td>
                    <td style="background-color:#E8E8E8">Co-Occurence</td>
                    <td style="background-color:#E8E8E8">0.8298</td>
                    <td>0.8984</td>
                    <td><b>0.948</b></td>
                    <td>0.9026</td>
                </tr>
                <tr>
                    <td>EN</td>
                    <td>DE</td>
                    <td style="background-color:#E8E8E8">Random Forest</td>
                    <td style="background-color:#E8E8E8">Co-Occurence</td>
                    <td style="background-color:#E8E8E8">0.6746<i><sub>t</sub></i></td>
                    <td>0.6777<i><sub>t</sub></i></td>
                    <td><b>0.8891</b></td>
                    <td>0.882</td>
                </tr>
                <tr>
                    <td>EN</td>
                    <td>EN</td>
                    <td style="background-color:#E8E8E8">Logistic Regression</td>
                    <td style="background-color:#E8E8E8">Co-Occurence</td>
                    <td style="background-color:#E8E8E8">0.8263</td>
                    <td>0.9167</td>
                    <td>0.9223</td>
                    <td><b>0.9284</b></td>
                </tr>
                <tr>
                    <td>EN</td>
                    <td>ES</td>
                    <td style="background-color:#E8E8E8">Logistic Regression</td>
                    <td style="background-color:#E8E8E8">Co-Occurence</td>
                    <td style="background-color:#E8E8E8">0.625<i><sub>t</sub></i></td>
                    <td>0.6767<i><sub>t</sub></i></td>
                    <td><b>0.8043</b></td>
                    <td>0.7725</td>
                </tr>
                <tr>
                    <td>EN</td>
                    <td>FR</td>
                    <td style="background-color:#E8E8E8">SVM</td>
                    <td style="background-color:#E8E8E8">Co-Occurence</td>
                    <td style="background-color:#E8E8E8">0.6732</td>
                    <td>0.7416</td>
                    <td><b>0.8002</b></td>
                    <td>0.7748</td>
                </tr>
                <tr>
                    <td>EN+DE</td>
                    <td>DE</td>
                    <td style="background-color:#E8E8E8">Logistic Regression</td>
                    <td style="background-color:#E8E8E8">Co-Occurence</td>
                    <td style="background-color:#E8E8E8">0.8146</td>
                    <td>0.9236</td>
                    <td>0.9397</td>
                    <td><b>0.9401</b></td>
                </tr>
                <tr>
                    <td>EN+DE</td>
                    <td>EN</td>
                    <td style="background-color:#E8E8E8">-</td>
                    <td style="background-color:#E8E8E8">-</td>
                    <td style="background-color:#E8E8E8">-</td>
                    <td>0.9258</td>
                    <td>0.9239</td>
                    <td><b>0.9275</b></td>
                </tr>
                <tr>
                    <td>EN+DE</td>
                    <td>ES</td>
                    <td style="background-color:#E8E8E8">-</td>
                    <td style="background-color:#E8E8E8">-</td>
                    <td style="background-color:#E8E8E8">-</td>
                    <td>0.7528</td>
                    <td><b>0.8467</b></td>
                    <td>0.8388</td>
                </tr>
                <tr>
                    <td>EN+DE</td>
                    <td>FR</td>
                    <td style="background-color:#E8E8E8">-</td>
                    <td style="background-color:#E8E8E8">-</td>
                    <td style="background-color:#E8E8E8">-</td>
                    <td>0.75</td>
                    <td>0.8298</td>
                    <td><b>0.8299</b></td>
                </tr>
            </table>
        </div>
    </div>
</div>


<br>

<h2>6 Experimental Results and Error Analysis</h2>
<!--CONTENT-->
<p>Insert Experimental Results and error analysis
<br>

<h3>6.1 Schema Matching Results</h3>

<p>
    As can be seen in table 3, TURL outperforms the baseline models as well as the contrastive learning approach.
    It seems as the number of epochs that TURL is trained on does not play an important role for the both mid sized
    and large dataset as the performance converges for both 25 and 50 epochs at the same outcomes. The size of the dataset
    seems to take an important part in the performance of the baseline models and TURL. The performances increase up to 20 percent points.
    For contrastive learning on the other hand, the size of the dataset only makes a difference of 0.01.
    To deepen the understanding of the differences between baselines and TURL, a more detailed evaluation with regards to the datatypes
    of the data is of interest. For example, while TinyBert and CL reach respectively a micro F1 Score of 0.5034 and 0.5780 for the type geolocation,
    TURL outperforms them with a micro F1 Score of 0.8857. Also floats and integers reach much higher micro F1 Scores with TURL.
    For datetime the performances of the three approaches are quite similar. Only the type string is detected more accurately
    by the baseline model and the CL approach.




    The necessary code to retrace our experimental runs can be found in our <a
            href="https://github.com/NiklasSabel/data_integration_using_deep_learning">GitHub repository</a>.
</p>

<div style="margin-top:2em;">
    <div style=";margin-right: 2.5em">
        <div class="tab">
            <button class="tablinks" onclick="openExpResult(event, 'unsup')">Schema</button>
            <button class="tablinks" onclick="openExpResult(event, 'supwc')">Entity</button>
        </div>
        <div id="unsup" class="tabcontent" style="display: none;">
            <table class="Multi-class">
                <caption style="margin-top: 1em">Table 3: Experimental results</caption>
                <tr>
                    <th colspan="1" style='text-align:center; vertical-align:middle'></th>
                    <th colspan="4" style='text-align:center; vertical-align:middle'> Baseline</th>
                    <th colspan="1" style='text-align:center; vertical-align:middle'>Other?</th>
                    <th colspan="1" style='text-align:center; vertical-align:middle'> Table-based Transformers</th>
                </tr>
                <tr>
                    <th>Train Set</th>
                    <th>Bert</th>
                    <th>RoBerta</th>
                    <th>Distilbert</th>
                    <th>TinyBert</th>
                    <th>Contrastive Learning</th>
                    <th>TURL</th>

                </tr>
                <tr>
                    <td>Small</td>
                    <td style="background-color:#E8E8E8">0.73</td>
                    <td style="background-color:#E8E8E8">0.66</td>
                    <td style="background-color:#E8E8E8">0.60</td>
                    <td style="background-color:#E8E8E8">0.69</td>
                    <td>style="background-color:#E8E8E8">0.75</td>
                    <td style="background-color:#E8E8E8">0.77</td>
                </tr>
                <tr>
                    <td>Medium</td>
                    <td style="background-color:#E8E8E8">0.76</td>
                    <td style="background-color:#E8E8E8">0.77</td>
                    <td style="background-color:#E8E8E8">0.62</td>
                    <td style="background-color:#E8E8E8">0.70</b></td>
                    <td>style="background-color:#E8E8E8">0.76</td>
                    <td style="background-color:#E8E8E8">0.81</td>
                </tr>
                <tr>
                    <td>Large</td>
                    <td style="background-color:#E8E8E8">0.80</td>
                    <td style="background-color:#E8E8E8">0.80</td>
                    <td style="background-color:#E8E8E8">0.80</td>
                    <td style="background-color:#E8E8E8">0.76</td>
                    <td>style="background-color:#E8E8E8">0.76</td>
                    <td style="background-color:#E8E8E8">0.87</td>
                </tr>
            </table>
        </div>
        <div id="supwc" class="tabcontent" style="display: none;">
            <table class="pair-wiseSmall">
                <caption style="margin-top: 1em">Table 3: Experimental results</caption>
                <tr>
                    <th>Category</th>
                    <th colspan="2" style='text-align:center; vertical-align:middle'> Setting</th>
                    <th colspan="3" style='text-align:center; vertical-align:middle'> Baseline</th>
                    <th colspan="3" style='text-align:center; vertical-align:middle'> Transformer</th>
                </tr>
                <tr>
                    <th></th>
                    <th>Train Lang</th>
                    <th>Test Lang</th>
                    <th>Classifier</th>
                    <th>Embedding</th>
                    <th>Score</th>
                    <th>BERT</th>
                    <th>mBERT</th>
                    <th>XLM-R</th>
                </tr>
                <tr>
                    <td rowspan="9" style='text-align:center; vertical-align:middle'>Toy</td>
                    <td>DE</td>
                    <td>DE</td>
                    <td style="background-color:#E8E8E8">SVM</td>
                    <td style="background-color:#E8E8E8">Co-Occurence</td>
                    <td style="background-color:#E8E8E8">0.8298</td>
                    <td>0.8984</td>
                    <td><b>0.948</b></td>
                    <td>0.9026</td>
                </tr>
                <tr>
                    <td>EN</td>
                    <td>DE</td>
                    <td style="background-color:#E8E8E8">Random Forest</td>
                    <td style="background-color:#E8E8E8">Co-Occurence</td>
                    <td style="background-color:#E8E8E8">0.6746<i><sub>t</sub></i></td>
                    <td>0.6777<i><sub>t</sub></i></td>
                    <td><b>0.8891</b></td>
                    <td>0.882</td>
                </tr>
                <tr>
                    <td>EN</td>
                    <td>EN</td>
                    <td style="background-color:#E8E8E8">Logistic Regression</td>
                    <td style="background-color:#E8E8E8">Co-Occurence</td>
                    <td style="background-color:#E8E8E8">0.8263</td>
                    <td>0.9167</td>
                    <td>0.9223</td>
                    <td><b>0.9284</b></td>
                </tr>
                <tr>
                    <td>EN</td>
                    <td>ES</td>
                    <td style="background-color:#E8E8E8">Logistic Regression</td>
                    <td style="background-color:#E8E8E8">Co-Occurence</td>
                    <td style="background-color:#E8E8E8">0.625<i><sub>t</sub></i></td>
                    <td>0.6767<i><sub>t</sub></i></td>
                    <td><b>0.8043</b></td>
                    <td>0.7725</td>
                </tr>
                <tr>
                    <td>EN</td>
                    <td>FR</td>
                    <td style="background-color:#E8E8E8">SVM</td>
                    <td style="background-color:#E8E8E8">Co-Occurence</td>
                    <td style="background-color:#E8E8E8">0.6732</td>
                    <td>0.7416</td>
                    <td><b>0.8002</b></td>
                    <td>0.7748</td>
                </tr>
                <tr>
                    <td>EN+DE</td>
                    <td>DE</td>
                    <td style="background-color:#E8E8E8">Logistic Regression</td>
                    <td style="background-color:#E8E8E8">Co-Occurence</td>
                    <td style="background-color:#E8E8E8">0.8146</td>
                    <td>0.9236</td>
                    <td>0.9397</td>
                    <td><b>0.9401</b></td>
                </tr>
                <tr>
                    <td>EN+DE</td>
                    <td>EN</td>
                    <td style="background-color:#E8E8E8">-</td>
                    <td style="background-color:#E8E8E8">-</td>
                    <td style="background-color:#E8E8E8">-</td>
                    <td>0.9258</td>
                    <td>0.9239</td>
                    <td><b>0.9275</b></td>
                </tr>
                <tr>
                    <td>EN+DE</td>
                    <td>ES</td>
                    <td style="background-color:#E8E8E8">-</td>
                    <td style="background-color:#E8E8E8">-</td>
                    <td style="background-color:#E8E8E8">-</td>
                    <td>0.7528</td>
                    <td><b>0.8467</b></td>
                    <td>0.8388</td>
                </tr>
                <tr>
                    <td>EN+DE</td>
                    <td>FR</td>
                    <td style="background-color:#E8E8E8">-</td>
                    <td style="background-color:#E8E8E8">-</td>
                    <td style="background-color:#E8E8E8">-</td>
                    <td>0.75</td>
                    <td>0.8298</td>
                    <td><b>0.8299</b></td>
                </tr>
                <tr style="border-top:2px solid grey">
                    <td rowspan="9" style='text-align:center; vertical-align:middle'>Phone</td>
                    <td>DE</td>
                    <td>DE</td>
                    <td style="background-color:#E8E8E8">SVM</td>
                    <td style="background-color:#E8E8E8">Co-Occurence</td>
                    <td style="background-color:#E8E8E8">0.7304</td>
                    <td>0.7507<i><sub>t</sub></i></td>
                    <td><b>0.8803</b></td>
                    <td>0.8498<i><sub>t</sub></i></td>
                </tr>
                <tr>
                    <td>EN</td>
                    <td>DE</td>
                    <td style="background-color:#E8E8E8">SVM</td>
                    <td style="background-color:#E8E8E8">Magellan</td>
                    <td style="background-color:#E8E8E8">0.6681</td>
                    <td>0.7168<i><sub>t</sub></i></td>
                    <td><b>0.8222</b></td>
                    <td>0.7607<i><sub>t</sub></i></td>
                </tr>
                <tr>
                    <td>EN</td>
                    <td>EN</td>
                    <td style="background-color:#E8E8E8">SVM</td>
                    <td style="background-color:#E8E8E8">Co-Occurence</td>
                    <td style="background-color:#E8E8E8">0.7571</td>
                    <td>0.7806<i><sub>t</sub></i></td>
                    <td><b>0.9117</b></td>
                    <td>0.8565<i><sub>t</sub></i></td>
                </tr>
                <tr>
                    <td>EN</td>
                    <td>ES</td>
                    <td style="background-color:#E8E8E8">Logistic Regression</td>
                    <td style="background-color:#E8E8E8">Magellan</td>
                    <td style="background-color:#E8E8E8">0.614</td>
                    <td>0.7292<i><sub>t</sub></i></td>
                    <td><b>0.7893</b></td>
                    <td>0.7556<i><sub>t</sub></i></td>
                </tr>
                <tr>
                    <td>EN</td>
                    <td>FR</td>
                    <td style="background-color:#E8E8E8">SVM</td>
                    <td style="background-color:#E8E8E8">Magellan</td>
                    <td style="background-color:#E8E8E8">0.5952<i><sub>t</sub></i></td>
                    <td>0.7218<i><sub>t</sub></i></td>
                    <td>0.7352</td>
                    <td><b>0.745<i><sub>t</sub></i></b></td>
                </tr>
                <tr>
                    <td>EN+DE</td>
                    <td>DE</td>
                    <td style="background-color:#E8E8E8">SVM</td>
                    <td style="background-color:#E8E8E8">Co-Occurence</td>
                    <td style="background-color:#E8E8E8">0.7297</td>
                    <td>0.8202<i><sub>t</sub></i></td>
                    <td><b>0.8874</b></td>
                    <td>0.8713</td>
                </tr>
                <tr>
                    <td>EN+DE</td>
                    <td>EN</td>
                    <td style="background-color:#E8E8E8">-</td>
                    <td style="background-color:#E8E8E8">-</td>
                    <td style="background-color:#E8E8E8">-</td>
                    <td>0.8462<i><sub>t</sub></i></td>
                    <td><b>0.9102</b></td>
                    <td>0.8713</td>
                </tr>
                <tr>
                    <td>EN+DE</td>
                    <td>ES</td>
                    <td style="background-color:#E8E8E8">-</td>
                    <td style="background-color:#E8E8E8">-</td>
                    <td style="background-color:#E8E8E8">-</td>
                    <td>0.7753<i><sub>t</sub></i></td>
                    <td><b>0.8091</b></td>
                    <td>0.7914</td>
                </tr>
                <tr>
                    <td>EN+DE</td>
                    <td>FR</td>
                    <td style="background-color:#E8E8E8">-</td>
                    <td style="background-color:#E8E8E8">-</td>
                    <td style="background-color:#E8E8E8">-</td>
                    <td>0.7795<i><sub>t</sub></i></td>
                    <td>0.7847<i><sub>t</sub></i></td>
                    <td><b>0.7951<i><sub>t</sub></i></b></td>
                </tr>
            </table>
        </div>
    </div>
</div>
<span id="toc7"></span>

<h2>7 Discussion and Outlook</h2>
<!--CONTENT-->
<p>
    Insert Discussion and Outlook
    <a style="color:#FF0000";>(Application Schema & Entity Matching together? (see slides Bizer: https://www.uni-mannheim.de/media/Einrichtungen/dws/Files_Teaching/Web_Data_Integration/HWS2021/WDI04-IdentityResolution-HWS2021.pdf))</a>.
</p>

<span id="toc8"></span>
<h2>8 References</h2>
<ol>
    <li>Deng, X. et al.:
        <a href="https://doi.org/10.48550/arXiv.2006.14806" target="_blank">TURL: Table Understanding through Representation Learning</a>. In:
       Proceedings of the VLDB Endowment 14, 3, 307--319 (2020).
    </li>
    <li>Iida, H. et al.:
        <a href="http://arxiv.org/abs/2105.02584" target="_blank">TABBIE: Pretrained Representations of Tabular Data</a>.
       arXiv:2105.02584 [cs] (2021).
    </li>
    <li>Devlin, J. et al.: <a
            href="http://dblp.uni-trier.de/db/conf/naacl/naacl2019-1.html#DevlinCLT19"
            target="_blank">BERT: Pre-training of Deep Bidirectional Transformers for Language
        Understanding</a>. In:
        Jill Burstein; Christy Doran & Thamar Solorio, ed., 'NAACL-HLT (1)', Association for Computational Linguistics,
        , pp. 4171-4186 (2019).
    </li>
    <li>Peeters, R., Bizer, C.: <a
            href="https://arxiv.org/abs/2202.02098"
            target="_blank">Supervised Contrastive Learning for Product Matching</a>.
       arXiv:2202.02098 [cs] (2022).
    </li>
    <li>Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, et al.: <a
        href="https://arxiv.org/abs/2004.11362"
        target="_blank">Supervised Contrastive Learning</a>.
        arXiv:2004.11362 [cs.LG] (2020).
    </li>
    <li>Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton.: <a
        href="https://arxiv.org/abs/2002.05709"
        target="_blank">A Simple Framework for Contrastive Learning of Visual Representations</a>.
        arXiv:2002.05709 [cs.LG] (2020)
    </li>
    <li>Tianyu Gao, Xingcheng Yao, and Danqi Chen.: <a
        href="https://arxiv.org/abs/2104.08821"
        target="_blank">SimCSE: Simple Contrastive Learning of Sentence Embeddings.</a>.
        arXiv:2104.08821 [cs.CL] (2021)
    </li>
    <li>Christophides, V. et al.: <a
            href="https://doi.org/10.2200/S00655ED1V01Y201507WBE013"
            target="_blank">Entity Resolution in the Web of Data</a>.
        In: Synthesis Lectures on the Semantic Web: Theory and Technology, 5(3):1–122, (2015).
    </li>
    <li>Vaswani, A. et al.: <a
            href="https://arxiv.org/abs/1706.03762"
            target="_blank">Attention Is All You Need</a>.
        In: NIPS'17: Proceedings of the 31st International Conference on Neural Information Processing Systems, 6000-6010 (2017).
    </li>
    <li>Jiao, X. et al.: <a
            href="https://arxiv.org/abs/1909.10351"
            target="_blank">TinyBERT: Distilling BERT for Natural Language Understanding</a>.
        In: Findings of the Association for Computational Linguistics: EMNLP 2020, 4163--4174, (2020).
    </li>
    <li>Liu, Y. et al.: <a
            href="http://arxiv.org/abs/1907.11692"
            target="_blank">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a>.
        arXiv:1907.11692 [cs], (2019).
    </li>
    <li>Joulin, A. et al.: <a
            href="https://arxiv.org/abs/1612.03651"
            target="_blank">FastText.zip: Compressing text classification models</a>.
        arXiv:1612.03651 [cs], (2016).
    </li>
    <li>Primpeli, A. et al.: <a
            href="https://dl.acm.org/doi/10.1145/3308560.3316609"
            target="_blank">The WDC Training Dataset and Gold Standard for Large-Scale Product Matching</a>.
        In:Proceedings of The 2019 World Wide Web Conference, 381–-386, San Francisco USA, (2019).
    </li>
    <li>Le, Quoc V., Mikolov, T.: <a
            href="https://arxiv.org/abs/1405.4053"
            target="_blank">Distributed Representations of Sentences and Documents</a>.
        arXiv:1405.4053 [cs], (2014).
    </li>
    <li>Yin, P. et al.: <a
            href="https://aclanthology.org/2020.acl-main.745"
            target="_blank">TaBERT: Pretraining for Joint Understanding of Textual and Tabular Data</a>.
        In: Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 8413–8426, (2020)
    </li>
    </ol>
    <span id="toc9"></span>
     <h2>9 Downloads</h2>
     <p>
        <a href="" target="_blank">Dataset for entity matching</a>
    </p>
    <p>
        <a href="" target="_blank">Dataset for schema matching</a>
    </p>


</div>
<script type="text/javascript">
    $('#toc').toc({
        'selectors': 'h2', //elements to use as headings
        'container': '#toccontent', //element to find all selectors in
        'smoothScrolling': true, //enable or disable smooth scrolling on click
        'prefix': 'toc', //prefix for anchor tags and class names
        'highlightOnScroll': true, //add class to heading that is currently in focus
        'highlightOffset': 100, //offset to trigger the next headline
        'anchorName': function (i, heading, prefix) { //custom function for anchor name
            return prefix + i;
        }
    });
    $('[id*="link_"]').each(function () {
        var element = $(this);
        element.click(function (e) {
            e.preventDefault();
            var id = element.attr("id").split("_")[1];
            element.parent().removeClass("show").addClass("no-show");
            $('#charts_' + id).removeClass("no-show").addClass("show");
        });
    });
    $('[id*="colapse_"]').each(function () {
        var element = $(this);
        element.click(function (e) {
            e.preventDefault();
            var id = element.attr("id").split("_")[1];
            element.parent().removeClass("show").addClass("no-show");
            $('#intro_' + id).removeClass("no-show").addClass("show");
        });
    });
    document.getElementById("defaultOpen").click();
</script>
</body>

</html>
